{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hr_anime_gan.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"q2uQKBDRSbEq","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","!nvcc --version"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BBWV4SuRShRn","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"w_fsOonBSkdx","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","\n","import custom_layers as op\n","import util\n","import dataset as dset"],"execution_count":0,"outputs":[]},{"metadata":{"id":"92QDqC8uSl4s","colab_type":"code","colab":{}},"cell_type":"code","source":["class generator(nn.Module):\n","\n","    def __init__(self, dim_noise, dim_label, dim_output_img=64, n_channel=3):\n","        super(generator, self).__init__()\n","\n","        inplace = True\n","        self.fc = nn.Linear(dim_noise + dim_label, 64 * (16 ** 2))\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu1 = nn.ReLU(inplace=inplace)\n","        self.block = nn.ModuleList([op.sr_resBlock(64) for _ in range(16)])\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.relu2 = nn.ReLU(inplace=inplace)\n","        self.sub_pixel_deconv2d = nn.ModuleList([op.sub_pixel_deconv2d(2, 64) for _ in range(3)])\n","        self.conv = nn.Conv2d(64, n_channel, 9, 1, 4)\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, nz, lb):\n","        x = torch.cat([nz, lb], 1)\n","        x = self.relu1(self.bn1(self.fc(x).view(-1, 64, 16, 16)))\n","        x_id = x\n","        for bk in self.block:\n","            x = bk(x)\n","        x = self.relu2(self.bn2(x)) + x_id\n","        for sub in self.sub_pixel_deconv2d:\n","            x = sub(x)\n","        x = self.tanh(self.conv(x))\n","        return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l2vxsfwoSqMn","colab_type":"code","colab":{}},"cell_type":"code","source":["class discriminator(nn.Module):\n","\n","    def __init__(self, dim_input_img=128, n_channel = 3, dim_label = 10):\n","        super(discriminator, self).__init__()\n","\n","        slope = 0.2\n","        inplace=True\n","        self.basic1 = nn.ModuleList([nn.Conv2d(n_channel, 32, 4, 2, 1), nn.LeakyReLU(slope, inplace)])\n","        self.basic1.extend([op.dis_resBlock(32, activate_before_addition=False) for _ in range(2)])\n","        self.basic2 = nn.ModuleList([nn.Conv2d(32, 64, 4, 2, 1), nn.LeakyReLU(slope, inplace)])\n","        self.basic2.extend([op.dis_resBlock(64) for _ in range(4)])\n","        self.basic3 = nn.ModuleList([nn.Conv2d(64, 128, 4, 2, 1), nn.LeakyReLU(slope, inplace)])\n","        self.basic3.extend([op.dis_resBlock(128) for _ in range(4)])\n","        self.basic4 = nn.ModuleList([nn.Conv2d(128, 256, 4, 2, 1), nn.LeakyReLU(slope, inplace)])\n","        self.basic4.extend([op.dis_resBlock(256) for _ in range(4)])\n","        self.basic5 = nn.ModuleList([nn.Conv2d(256, 512, 4, 2, 1), nn.LeakyReLU(slope, inplace)])\n","        self.basic5.extend([op.dis_resBlock(512) for _ in range(4)])\n","        self.basic6 = nn.ModuleList([nn.Conv2d(512, 1024, 3, 2, 1), nn.LeakyReLU(slope, inplace)])\n","        self.block = nn.ModuleList([self.basic1, self.basic2, self.basic3, self.basic4, self.basic5, self.basic6])\n","        num_reduce_half = 6\n","        dim_final_kernel = int(dim_input_img / (2 ** num_reduce_half))\n","        self.flatten_size = 1024 * (dim_final_kernel ** 2)\n","        self.fc_score = nn.Linear(self.flatten_size, 1)\n","        self.fc_label = nn.Linear(self.flatten_size, dim_label)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        for bk in self.block:\n","            for bs in bk:\n","                x = bs(x)\n","        x = x.view(-1, self.flatten_size)\n","        x_score = self.sig(self.fc_score(x))\n","        x_label = self.sig(self.fc_label(x))\n","\n","        return x_score, x_label"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PLa2VL2LSvAX","colab_type":"code","colab":{}},"cell_type":"code","source":["def generate_random_label(num_fixed_ns_img, dim_label, device):\n","    return torch.randn(num_fixed_ns_img, dim_label, device=device)\n","\n","def dragan_penalty(discriminator, input, scale, k, device):\n","    b_size = input.size(0)\n","    print(input.size())\n","    alpha = torch.randn(b_size, 1, 1, 1).expand(input.size())\n","    noise = 0.5 * input.std() * torch.randn(input.size())\n","    input_nz = input + alpha * noise\n","    input_nz.requires_grad_(True)\n","    output_nz, _ = discriminator(input_nz)\n","    grad = autograd.grad(output_nz, input_nz, torch.ones(output_nz.size()).to(device), \\\n","        create_graph=True, retain_graph=True, only_inputs=True)[0]\n","    penalty = scale * ((grad.norm(2, dim=1) - k)** 2).mean()\n","    return penalty"],"execution_count":0,"outputs":[]},{"metadata":{"id":"g99P8b6OSzMM","colab_type":"code","colab":{}},"cell_type":"code","source":["def train_base(epochs, batch_size, dim_noise, dim_label, device, dataset, generator, discriminator, loss, loss_class, optimizer_gen, optimizer_dis, filepath=None):\n","    # load the data\n","    worker = 2\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=worker)\n","    \n","    # create the list to store each loss\n","    loss_list, score_list, img_list = [], [], []\n","    num_fixed_ns_img = 64\n","    fixed_noise = torch.randn(num_fixed_ns_img, dim_noise, device=device)\n","    fixed_label = generate_random_label(num_fixed_ns_img, dim_label, device=device)\n","\n","    # start iterating the epoch\n","    for e in range(epochs):\n","        loss_dis, loss_gen, score_dis_real, score_dis_fake, score_gen = 0, 0, 0, 0, 0\n","\n","        for i, data in enumerate(dataloader):\n","            b_size = batch_size\n","            if len(data[0]) < batch_size:\n","                b_size = len(data[0])\n","            # ---------------------------\n","            # 1. Train the discriminator\n","            # ---------------------------\n","            # generate noise samples from the generator\n","            batch_noise = torch.randn(b_size, dim_noise, device=device)\n","            batch_label = generate_random_label(b_size, dim_label, device=device)\n","            fake_data = generator(batch_noise, batch_label)\n","\n","            # start to train the discriminator\n","            discriminator.zero_grad()\n","            # calculate the loss of the noise samples, which assigns the same label 0\n","            # for all the samples, and get the single output(marks) from the discriminator\n","            output, output_label = discriminator(fake_data.detach())  # use .detach() to stop the requirement of gradient\n","            output = output.view(-1)\n","            class_label = torch.full((b_size,), 0, device=device)\n","\n","            loss_d_ns_adv = loss(output, class_label)\n","            loss_d_ns_cls = loss_class(output_label, batch_label)\n","            lambda_adv = dim_label\n","            loss_d_ns = lambda_adv * loss_d_ns_adv + loss_d_ns_cls\n","            loss_d_ns.backward()\n","            score_dis_fake = output.mean().item()\n","            \n","            # calculate the loss of the real samples and assigns label 1 to represent\n","            # all samples are true and get the single output(marks) from the discriminator\n","            real_data = data[0].to(device)\n","            real_label = data[1][0]\n","            output, output_label = discriminator(real_data)\n","            output = output.view(-1)\n","            class_label.fill_(1)\n","            loss_d_real_adv = loss(output, class_label)\n","            loss_d_real_cls = loss_class(output_label, real_label)\n","            loss_d_real = lambda_adv * loss_d_real_adv + loss_d_real_cls\n","            loss_d_real.backward()\n","            loss_d_penelty = dragan_penalty(discriminator, real_data, lambda_adv, 1, device)\n","            loss_d_penelty.backward()\n","            loss_d = loss_d_ns + loss_d_real + loss_d_penelty\n","            score_dis_real = output.mean().item()\n","            loss_dis = loss_d.item()\n","            optimizer_dis.step()\n","\n","            # ---------------------------\n","            # 2. Train the generator\n","            # ---------------------------\n","            # Feed the noise samplea to the discriminator agian to geit the accurate scores\n","            # after training the discriminator, and assign label 1 not to see the noise as\n","            # real label but to let the loss function to be correct and do correct back propogation\n","            generator.zero_grad()            \n","            # batch_noise = Func.torch.randn(b_size, dim_noise)\n","            # fake_data = generator(batch_noise)\n","            output, output_label = discriminator(fake_data)\n","            output = output.view(-1)\n","            loss_g_adv = loss(output, class_label)\n","            loss_g_cls = loss_class(output_label, batch_label)\n","            loss_g = lambda_adv * loss_g_adv + loss_g_cls\n","            loss_g.backward()\n","            score_gen = output.mean().item()\n","            loss_gen = loss_g.item()\n","            optimizer_gen.step()\n","\n","\n","            # print information to the console\n","            # print information 5 times in a epoch\n","            num2print = 30\n","            if (i + 1) % num2print == 0:\n","                print('epoch: %d, iter: %d, loss_D: %.4f, loss_G: %.4f;\\t Scores: train D: D(x): %.4f, D(G(z)): %.4f train G: D(G(z))： %.4f'\n","                        % (e, (i + 1), loss_dis, loss_gen, score_dis_real, score_dis_fake, score_gen))           \n","                \n","                # store the final loss for D and G for a specific time interval of a whole epoch\n","                loss_list.append([loss_dis, loss_gen])\n","                # store the final score from D for noise and real samples for a specific time imterval on current epoch\n","                score_list.append([score_dis_fake, score_dis_real, score_gen])\n","\n","        loss_list.append([loss_dis, loss_gen])\n","        score_list.append([score_dis_fake, score_dis_real, score_gen])\n","        # store the image that the generator create for each epoch\n","        test_img = generator(fixed_noise, fixed_label)\n","        test_img = test_img.detach().cpu()\n","        img_list.append(test_img.numpy())\n","\n","        # save the model\n","        if (e + 1) % 5 == 0:\n","            util.save_checkpoint(e, generator, discriminator, filepath)\n","    \n","    loss_list = list(map(list, zip(*loss_list)))\n","    score_list = list(map(list, zip(*score_list)))\n","        \n","    return generator, discriminator, loss_list, score_list, img_list"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4bVnZa7eTA50","colab_type":"code","colab":{}},"cell_type":"code","source":["def build_gen_dis(config):\n","    net_gen = generator(config.DIM_NOISE, config.N_LABEL, config.DIM_IMG, config.N_CHANNEL).to(config.DEVICE)\n","    net_dis = discriminator(config.DIM_IMG, config.N_CHANNEL, config.N_LABEL).to(config.DEVICE)\n","\n","    if config.INIT:\n","        net_gen.apply(init_weight)\n","        net_dis.apply(init_weight)\n","    else:\n","        ext = config.PATH_MODEL[-4]\n","        path_model = config.PATH_IMPORT_MODEL[:-4] + '_epoch_%d' + ext % config.IMPORT_IDX_EPOCH\n","        net_gen, net_dis = util.load_checkpoint(config.EPOCHS, net_gen, net_dis, path_model)\n","\n","    return net_gen, net_dis\n","\n","def train(dataset, net_gen, net_dis, config):\n","\n","    # config = config.config_illustration_gan\n","    loss_class = nn.BCEWithLogitsLoss().to(config.DEVICE)\n","    loss_label = nn.MultiLabelSoftMarginLoss().to(config.DEVICE)\n","\n","    optim_gen = optim.Adam(net_gen.parameters(), lr=config.LEARNING_RATE, betas=(config.MOMENTUM, 0.99))\n","    optim_dis = optim.Adam(net_dis.parameters(), lr=config.LEARNING_RATE, betas=(config.MOMENTUM, 0.99))\n","\n","    net_gen, net_dis, losses, _, imgs = train_base(config.EPOCHS, config.BATCH_SIZE, config.DIM_NOISE, config.N_LABEL, config.DEVICE,\n","                                                    dataset, net_gen, net_dis, loss_class, loss_label, optim_gen, optim_dis, config.PATH_MODEL)\n","    \n","    return net_gen, net_dis, losses, imgs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XazteOC6TB1c","colab_type":"code","colab":{}},"cell_type":"code","source":["def init_weight(layer):\n","    std = 0.02\n","    if type(layer) == nn.ConvTranspose2d:\n","        nn.init.normal_(layer.weight.data, mean=0, std=std)\n","    elif type(layer) == nn.Conv2d:\n","        nn.init.normal_(layer.weight.data, mean=0, std=std)\n","    elif type(layer) == nn.Linear:\n","        nn.init.normal_(layer.weight.data, mean=0, std=std)\n","        nn.init.normal_(layer.bias.data, mean=0, std=std)\n","    elif type(layer) == nn.BatchNorm2d:\n","        nn.init.normal_(layer.weight.data, mean=1, std=std)\n","        nn.init.constant_(layer.bias.data, 0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Fmz4HghYTWJn","colab_type":"code","colab":{}},"cell_type":"code","source":["PATH_IMAGE = '/content/gdrive/My Drive/data/image_64'\n","PATH_TAG = '/content/gdrive/My Drive/data/tags'\n","ARTWORK_TYPE = os.listdir(PATH_IMAGE)\n","IS_ADD_I2V_TAG = False\n","\n","class config_hr_anime_gan():\n","    PATH_MODEL = '/content/gdrive/My Drive/data/model/illust_gan_64.pth'\n","    IS_ADD_I2V_TAG = True\n","    BATCH_SIZE = 64\n","    DIM_IMG = 64\n","    DIM_NOISE = 100\n","    LEARNING_RATE = 0.0002\n","    MOMENTUM = 0.5\n","    EPOCHS = 10\n","    INIT = True\n","    IMPORT_IDX_EPOCH = EPOCHS\n","    DEVICE = torch.device(\"cuda:0\")\n","    N_CHANNEL = 3"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NvIuzJTGTlxK","colab_type":"code","colab":{}},"cell_type":"code","source":["f_minst = dset.fmnist('content/gdrive/My Drive/data/Fashion-MNIST', download=True, image_size=config_illustration_gan.DIM_IMG)\n","\n","transform=transforms.Compose([transforms.Resize(64), transforms.ToTensor()])\n","anime = dset.animeFaceDataset('/content/gdrive/My Drive/data/anime-faces', transform=transform)\n","\n","dataset = dset.pokemonDataset(PATH_IMAGE, PATH_TAG, ARTWORK_TYPE, IS_ADD_I2V_TAG)\n","\n","\"\"\"\n","mean, std = get_channel_mean_std(dataset, DIM_IMG)\n","     \n","print(mean)\n","print(std)\n","\"\"\"\n","mean = [220.43362509, 217.50907014, 212.78514176]\n","std = [71.7985852,  73.64374336, 78.23258064]\n","transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n","dataset.set_transform(transform)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CXcgKT1NTo52","colab_type":"code","colab":{}},"cell_type":"code","source":["CONFIG = config_hr_anime_gan\n","net_gen, net_dis = build_gen_dis(CONFIG)\n","print(net_gen)\n","print(net_dis)\n","net_gen, net_dis, losses, imgs = train(anime, net_gen, net_dis, CONFIG)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"W7R3cAFdTyDX","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.figure(figsize=(10, 5))\n","plt.plot(losses[0], label = 'generator')\n","plt.plot(losses[1], label = 'discriminator')\n","plt.title('Loss of training the gennerator and discriminator')\n","plt.xlabel('loss')\n","plt.ylabel('process')\n","plt.legend()\n","plt.show()\n","\n","for i in range(CONFIG.EPOCHS + 1):\n","    grid_img = util.make_figure_grid(imgs[i], 8)\n","    plt.figure()\n","    plt.imshow(grid_img)\n","    plt.show()"],"execution_count":0,"outputs":[]}]}